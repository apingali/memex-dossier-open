'''Advanced Message Queue Protocol (AMQP) implemented using RabbitMQ

.. Your use of this software is governed by your license agreement.
   Copyright 2017 Diffeo, Inc.

'''
from __future__ import absolute_import, division, print_function
import abc
import argparse
from collections import Counter, defaultdict
import functools
from itertools import imap, islice
import json
import logging
import os
import threading
import time
import traceback
import uuid
import urllib
import pika

import coordinate
from dossier.label import RelationLabelStore
from memex_dossier.fc import StringCounter, FeatureCollection
from ..web.filters import already_labeled, geotime
#from .dossier.web.util import fc_to_json
from .web.config import Config
from .etl.interface import create_fc_from_html

import requests

# Needed only for yakonfig bootstrapping:
import dblogger
import kvlayer
import yakonfig

LOGGER = logging.getLogger(__name__)


def get_recommendations(fc, config, limit=10, engine_name=None):
    '''"next link" recommendations for the Web Navigator prototype

    General 

    '''
    if engine_name == 'twitter_cluster':
        return  # twitter_cluster_query(fc, config, limit)

    if engine_name == 'company_profile':
        return list(islice(ticker_symbol_query(fc, config, limit), limit))

    crossing_feature_names = set([
        'Person', 'Organization', 'Phone', 'Email', 'Username'])
    candidates = []

    queries = {}
    for name in fc['NAME']:
        queries = {name: {'origin_type': 'NAME', 'origin_name': name}}
        for feature_name in fc:
            if feature_name not in crossing_feature_names:
                continue
            for other_name in fc[feature_name]:
                parts = sorted([name, other_name])
                #LOGGER.info(parts)
                query = '''"%s" "%s"''' % tuple(parts)
                queries[query] = {'origin_type': feature_name,
                                  'origin_name': other_name}

    engines = {
        'google': config.google.web_search_with_paging,
        'cdr': cdr_query,
        'onion': onion_query,
        }
    
    engine = engines.get(engine_name)
    for query in queries:
        for _it in engine(query, limit=10, tfidf=config.tfidf):
            item = {}
            item['title'] = _it['title']
            item['url'] = _it['link']
            item['snippet'] = _it['htmlSnippet']
            item['fc'] = _it.get('fc', {})
            item.update(queries[query])
            if 'origin_type' in _it and 'origin_name' in _it:
                item['origin_type'] = _it['origin_type']
                item['origin_name'] = _it['origin_name']
            
            candidates.append(item)
            if len(candidates) >= limit:
                break
        if len(candidates) >= limit:
            break

    return candidates


# using the twitter-cluster.json file of static data generated by
# Hyperion in 2017 Spring QPR

# twitter_cluster_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'twitter-clusters.json')
# raw_twitter_clusters = json.load(open(twitter_cluster_path))
# twitter_cluster = defaultdict(list)
# for cluster in raw_twitter_clusters:
#     for symbol in cluster['symbols']:
#         twitter_cluster[symbol.lower()].append(cluster)
#     for user in cluster['users']:
#         twitter_cluster[user.lower()].append(cluster)
    

# def twitter_cluster_query(fc, config, limit=20):
#     '''if any string from the incoming query FC hits on any cluster in the
#     twitter-cluster data, then send back the cluster as an HTML
#     formatted snippet.

#     '''
#     strs = Counter()
#     for fname, sc in fc.iteritems():
#         strs += sc
#     clusters = defaultdict(list)
#     for s, _ in strs.most_common():
#         s = s.lower()
#         if s in twitter_cluster:
#             clusters[s].extend(twitter_cluster[s])
#     candidates = []
#     for hit, clust_list in clusters.iteritems():
#         real_hit = None
#         real_link = None
#         user_links = set()
#         symbol_links = set()

#         for clust in clust_list:
#             for symbol in clust['symbols']:
#                 link = 'https://www.otcmarkets.com/stock/%s/profile' % symbol
#                 if symbol.lower() == hit:
#                     real_hit = symbol
#                     real_link = link
#                 else:
#                     symbol_links.add(('Organization', symbol, link))
#             for user in clust['users']:
#                 link = 'https://twitter.com/' + user
#                 if user.lower() == hit:
#                     real_hit = user
#                     real_link = link
#                 else:
#                     user_links.add(('Person', user, link))

#         for origin_type, anchor, link in sorted(user_links) + sorted(symbol_links):
#             # snippet = ' '.join(['<a href="%s">%s</a>' % (link, anchor)
#             #                     for anchor, link in sorted(user_links) + sorted(symbol_links)])
#             item = {}
#             item['origin_name'] = real_hit
#             item['origin_type'] = origin_type
#             item['title'] = 'linked to %s' % anchor
#             item['url'] = link
#             item['snippet'] = real_link
#             candidates.append(item)
#             if len(candidates) > limit: break
#         if len(candidates) > limit: break
#     return candidates


def ticker_symbol_query(fc, config, limit=20):
    '''if any string from the incoming query FC hits on any cluster in the
    twitter-cluster data, then send back the cluster as an HTML
    formatted snippet.

    '''
    strs = Counter()
    for fname, sc in fc.iteritems():
        strs += sc
    tickers = []
    for s, _ in strs.most_common():
        if s.upper() == s and 3 <= len(s) < 10:
            tickers.append(s)
    pairs = [
        (
            'https://www.otcmarkets.com/stock/%s/profile',
            'OTC Markets Company Profile'),
        (
            'http://investorshub.advfn.com/boards/getboards.aspx?searchstr=%s',
            'Investors Hub Boards Profile'),
        (
            'http://www.theotc.today/p/daily-trades-report.html?%s',
            'The OTC Today Past Performance'),
        (
            'https://www.google.com/finance?q=%s',
            'Google Finance Profile'),
        (
            'http://finance.yahoo.com/quote/%s',
            'Yahoo Finance Profile'),
    ]
    for symbol in tickers:
        for link_pattern, title in pairs:
            item = {}
            item['origin_name'] = symbol
            item['origin_type'] = 'Organization'
            item['title'] = title
            item['url'] = link_pattern % symbol
            yield item


def cdr_query(query, limit=10, tfidf=None):
    url = 'http://cdr-microcap-2:fFxYGMHx2MS39Cz2@54.68.119.208:9200/memex-domains/_search?q=raw_content:%s&from=0&size=10' % urllib.quote(query)

    resp = requests.get(url)
    data = resp.json()
    #LOGGER.info(json.dumps(data, indent=4, sort_keys=True))
    for hit in data['hits']['hits']:
        url = hit['_source']['url']
        html = hit['_source']['raw_content']
        try:
            fc = create_fc_from_html(url, html, encoding=None, tfidf=tfidf)
        except Exception as exc:
            LOGGER.warn('failed on fc construction from %r', url, exc_info=True)
            continue
        if not fc:
            LOGGER.warn('failed on fc construction from %r', url, exc_info=True)
            continue
        #LOGGER.info('continuing with %r', url)
        feature_names_for_snippet = ['usernames', 'skype', 'phone', 'email', 'PERSON', 'ORGANIZATION', 'bowNP_unnorm', 'bowNP_sip']
        features = Counter()
        if not fc:
            return
        for fname in feature_names_for_snippet:
            if fname in fc:
                for feature_str in fc[fname]:
                    features[feature_str] += 1
        #LOGGER.info(json.dumps(features.most_common(), indent=4, sort_keys=True))
        def more_longer(x, y):
            if x[1] == y[1]:
                return cmp(len(x[0]), len(y[0]))
            else:
                return cmp(x[1], y[1])
        sorted_features = sorted(features.items(), cmp=more_longer, reverse=True)
        snippet = ' '.join([s for s, c in sorted_features])
        snippet = snippet[:1000]
        
        # this list is ordered by coolness factor and used to select the origin_name if possible....
        feature_names_for_origin_name = ['usernames', 'skype', 'phone', 'email', 'PERSON', 'ORGANIZATION']
        origin_name = None
        origin_type = None
        # for fname in feature_names_for_origin_name:
        #     if fname

        _it = {}
        _it['link'] = url
        title_counter = fc.get('title', {})
        if title_counter:
            _it['title'] = title_counter.keys()[0]
        else:
            _it['title'] = '(missing title)'
        _it['htmlSnippet'] = snippet
        #_it['fc'] = FeatureCollection.to_dict(fc)
        yield _it


def onion_query(query, limit=10, tfidf=None):

    # EXAMPLE_JSON:    {"KeyTerm": "CartelNorteAfrica", "RegistrationKey":   "MyDogAteMyKey", "Action": "fetch", "MaxRecordCnt": "500"}  .... -->   fields       Please use "MyDogAteMyKey" for this week's QPR.     Action = [count|fetch],   Keyterm = [a persona name},  MaxRecordCnt = [1..1000].

    #url = 'http://cdr-microcap-2:fFxYGMHx2MS39Cz2@54.68.119.208:9200/memex-domains/_search?q=raw_content:%s&from=0&size=10' % urllib.quote(query)

    auth = ('memex', 'MemexHS2014')
    url = 'https://memex1.csl.sri.com/lights_api'
    headers = {"Content-Type": "application/json"}

    resp = requests.post(url, headers=headers, auth=auth)
    # TODO: poll for the resp.json()['URL'] until it appears, maybe
    # half an hour latter, and then digest that.
    return []


def main():
    '''Launch the AMQP worker.'''
    filters = {
        'already_labeled': already_labeled,
        'geotime': geotime,
    }

    worker = AMQPWorker(filters)
    parser = argparse.ArgumentParser()
    modules = [yakonfig, kvlayer, dblogger, coordinate, worker]
    yakonfig.parse_args(parser, modules)
    worker.task_master = coordinate.TaskMaster(
        yakonfig.get_global_config(coordinate.config_name))

    worker.start()


#: Name of the AMQP recommendations exchange.
RECOMMENDATIONS_EXCHANGE = 'recommendations'

#: Routing key for recommendation results.
RESULT_KEY = 'recommendations.result'

#: Name of the AMQP session exchange.
SESSION_EXCHANGE = 'session'

#: Routing key prefix for session events.
SESSION_EVENT_KEY_PREFIX = 'session.event.'

#: Name of the AMQP dead-letter exchange.
DEAD_LETTER_EXCHANGE = 'dlx'


class AMQPWorker(object):
    '''Top-level worker application object.'''

    # :mod:`yakonfig` metadata:
    config_name = 'amqp'
    default_config = {'url': None}

    @staticmethod
    def add_arguments(parser):
        '''Add arguments to a :mod:`argparse` parser.'''
        parser.add_argument('--amqp', metavar='URL',
                            help='amqp://... URL to broker')

    runtime_keys = {'amqp': 'url'}

    @staticmethod
    def discover_config(config, name):
        '''Fill in extra config from the environment.'''
        if not config.get('url') and 'AMQP_URL' in os.environ:
            config['url'] = os.environ['AMQP_URL']

    def normalize_config(self, config):
        '''Capture information from the configuration.'''
        self.url = config['url']

    @staticmethod
    def check_config(config, name):
        '''Validate a potential configuration.'''
        if not config.get('url'):
            raise yakonfig.ConfigurationError(name + ' "url" is required')

    def __init__(self, filters, task_master=None, url=None, kvl=None):
        '''Create a new :class:`AMQPWorker`.'''
        #: :module:`memex_dossier.web` configuration factory.
        #self.config = config

        #: #:module:`coordinate` job queue client.
        self.task_master = task_master

        #: Dictionary of :module:`memex_dossier.web` filters.
        self.filters = filters

        #: URL to the AMQP server.
        self.url = url

        #: Persistent AMQP connection.
        self.connection = None

        #: Current AMQP channel.
        self.channel = None

        #: Has the recommendations exchange been declared?
        self._recommendations_declared = False

        #: Has the session exchange been declared?
        self._session_declared = False

        #: Recommendation tracker.

    def start(self):
        '''Start all of the exciting AMQPness.'''
        # Connect to RabbitMQ
        parameters = pika.URLParameters(self.url)
        connection = pika.SelectConnection(parameters, self.on_connected)

        # Main loop:
        try:
            connection.ioloop.start()
        except KeyboardInterrupt:
            # shut down gracefully
            connection.close()
            connection.ioloop.start()

    def on_connected(self, connection):
        '''Callback on initial AMQP connection.'''
        self.connection = connection
        self.connection.channel(self.on_channel_open)

    def on_channel_open(self, channel):
        '''Callback when an AMQP channel is opened.'''
        self.channel = channel
        self.channel.basic_qos(prefetch_count=1)
        self.channel.exchange_declare(
            callback=self.on_recommendations_declared,
            exchange=RECOMMENDATIONS_EXCHANGE,
            exchange_type='topic',
            durable=True,
            arguments={'alternate-exchange': DEAD_LETTER_EXCHANGE},
        )
        # self.channel.exchange_declare(
        #     callback=self.on_ingest_declared,
        #     exchange=INGEST_EXCHANGE,
        #     exchange_type='topic',
        #     durable=True,
        #     arguments={'alternate-exchange': DEAD_LETTER_EXCHANGE},
        # )
        self.channel.exchange_declare(
            callback=self.on_session_declared,
            exchange=SESSION_EXCHANGE,
            exchange_type='topic',
            durable=True,
            arguments={'alternate-exchange': DEAD_LETTER_EXCHANGE},
        )

    def on_recommendations_declared(self, _):
        '''Callback when the recommendations exchange is declared.'''
        self._recommendations_declared = True
        self.on_exchange_declared(_)

    def on_session_declared(self, _):
        '''Callback when the session exchange is declared.'''
        self._session_declared = True
        self.on_exchange_declared(_)

    def on_exchange_declared(self, _):
        '''Callback when an AMQP exchange is declared.'''
        # We need both exchanges to be up to do anything.
        if not (self._recommendations_declared and self._session_declared):
            return
        Tier0Worker(self.connection, self.config, self.filters).start()


class QueueWorker(object):
    '''Generic AMQP queue worker.'''

    __metaclass__ = abc.ABCMeta

    @abc.abstractproperty
    def exchange_name(self):
        '''Name of the exchange this connects to.'''
        return ''

    @abc.abstractproperty
    def routing_key(self):
        '''Routing key to use when binding.'''
        return ''

    @abc.abstractproperty
    def queue_name(self):
        '''Name of the queue this serves.'''
        return ''

    @property
    def app_id(self):
        '''Service name to advertise in outbound messages.'''
        return self.queue_name

    @abc.abstractmethod
    def handle(self, channel, method, properties, body):
        '''Handle an inbound message.'''
        pass

    def __init__(self, connection):
        '''Create a new worker on an existing AMQP connection.'''
        self.connection = connection
        self.channel = None

    def start(self):
        '''Create queues, register callbacks, and start work.'''
        self.connection.channel(self.on_channel_open)

    def on_channel_open(self, channel):
        '''Callback when an AMQP channel is opened.'''
        self.channel = channel
        self.channel.basic_qos(prefetch_count=1)
        self.channel.queue_declare(
            callback=self.on_queue_declared,
            queue=self.queue_name,
            durable=True,
            arguments={'x-message-ttl': 60000,
                       'x-dead-letter-exchange': DEAD_LETTER_EXCHANGE},
        )

    def on_queue_declared(self, _):
        '''Callback when an AMQP queue is declared.'''
        self.channel.basic_consume(
            self.on_delivery,
            queue=self.queue_name,
        )
        self.channel.queue_bind(
            callback=None,
            queue=self.queue_name,
            exchange=self.exchange_name,
            routing_key=self.routing_key,
        )

    def on_delivery(self, channel, method, properties, body):
        '''Callback when a message has been received.

        :param channel: AMQP channel
        :type channel: :class:`pika.Channel`
        :param method: Message protocol details
        :type method: :class:`pika.spec.Basic.Deliver`
        :param properties: Message metadata
        :type properties: :class:`pika.spec.BasicProperties`
        :param str body: Message body

        '''
        LOGGER.info('received message exchange=%s routing_key=%s '
                    'message_id=%s correlation_id=%s length=%d',
                    method.exchange, method.routing_key,
                    properties.message_id, properties.correlation_id,
                    len(body))

        try:
            self.handle(channel, method, properties, body)
        except Exception:
            LOGGER.error('error handling message correlation_id=%s',
                         properties.correlation_id, exc_info=True)
        finally:
            LOGGER.info('acknowledged message message_id=%s correlation_id=%s',
                        properties.message_id, properties.correlation_id)
            channel.basic_ack(method.delivery_tag)


def session_logged(f):
    '''Wrap a handler function, logging to the session service.

    This publishes a "begin" event to the session bus, then calls the
    wrapped handler.  Sends either an "end" or "fail" event to the
    session bus depending on whether or not it raised an exception.
    Captures all exceptions in executing the handler.

    '''
    @functools.wraps(f)
    def wrapper(self, channel, method, properties, body):
        self.channel.basic_publish(
            SESSION_EXCHANGE,
            SESSION_EVENT_KEY_PREFIX + 'begin',
            '',
            properties=pika.spec.BasicProperties(
                delivery_mode=2,
                correlation_id=properties.correlation_id,
                app_id=self.app_id,
                message_id=str(uuid.uuid4()),
                timestamp=int(time.time()),
            )
        )

        try:
            f(self, channel, method, properties, body)
            self.channel.basic_publish(
                SESSION_EXCHANGE,
                SESSION_EVENT_KEY_PREFIX + 'end',
                '',
                properties=pika.spec.BasicProperties(
                    delivery_mode=2,
                    correlation_id=properties.correlation_id,
                    app_id=self.app_id,
                    message_id=str(uuid.uuid4()),
                    timestamp=int(time.time()),
                )
            )
        except Exception as e:
            LOGGER.error('exception servicing message', exc_info=True)
            err = {
                'error': {
                    'code': 500,
                    'error': 'internal',
                    'message': str(e),
                    'details': {
                        'traceback': traceback.format_exc()
                    }
                }
            }
            self.channel.basic_publish(
                SESSION_EXCHANGE,
                SESSION_EVENT_KEY_PREFIX + 'fail',
                json.dumps(err),
                properties=pika.spec.BasicProperties(
                    content_type='application/vnd.diffeo.error.v1+json',
                    delivery_mode=2,
                    correlation_id=properties.correlation_id,
                    app_id=self.app_id,
                    message_id=str(uuid.uuid4()),
                    timestamp=int(time.time()),
                )
            )
    return wrapper


class RecommenderWorker(QueueWorker):
    '''Abstract base class for workers that produce recommendations.

    All recommenders need to do essentially the same things: parse the
    original query and filter terms, post-filter the recommended
    mentions, and apply scores and formatting.  This base class does
    the unrewarding support work.

    '''

    @property
    def exchange_name(self):
        '''Name of the exchange this connects to.'''
        return RECOMMENDATIONS_EXCHANGE

    @property
    def routing_key(self):
        '''Routing key to use when binding.'''
        return FC_KEY

    def __init__(self, connection, config, filters):
        '''Create a new inline recommendation worker.'''
        super(RecommenderWorker, self).__init__(connection)
        self.config = config
        self.filters = filters

    @staticmethod
    def filters_from_params(params):
        '''Create a filter parameter dictionary from query parameters.'''
        filters = {}
        return filters

    @session_logged
    def handle(self, channel, method, properties, body):
        '''Make and present recommendations.'''
        response = self.make_response(properties.correlation_id, body)
        if response is not None:
            self.channel.basic_publish(
                RECOMMENDATIONS_EXCHANGE,
                RESULT_KEY,
                json.dumps(response),
                properties=pika.spec.BasicProperties(
                    content_type='application/vnd.diffeo.document.v1+json',
                    delivery_mode=2,
                    correlation_id=properties.correlation_id,
                    app_id=self.app_id,
                    message_id=str(uuid.uuid4()),
                    timestamp=int(time.time()),
                ),
            )

    def make_response(self, correlation_id, body):
        '''Run the relevant part of the recommendation framework.'''
        data = json.loads(body)
        fc = IngestResponseFeatureCollections.fc_from_wire(data['fc'])
        request = data['request']
        params = request['query_params']
        filter_kwargs = self.filters_from_params(params)
        query_id = str(request['id'])
        query_vertex = MockVertex(query_id, fc)

        if not self.precheck_filters(filter_kwargs, correlation_id):
            return None

        # Create the unified filter predicate.
        search_engine = (
            self.config.create()
            .set_query_id(query_id)
            .set_query_params(params))

        for name, filter_ in self.filters.items():
            search_engine.add_filter(name, self.config.create(filter_))

        pred = search_engine.create_filter_predicate()

        # Actually get recommendations.
        results = self.get_recommendations(query_id, fc, filter_kwargs)

        # Compute scores as needed
        for result in results:
            result_vertex = MockVertex(r['content_id'], r['fc'])
            # This score will get overwritten by the merger later
            result['score'] = result['confidence']
            if search_engine.params['omit_fc']:
                del result['fc']
            else:
                result['fc'] = fc_to_json(result['fc'])

        # Sort the results by score (confidence/similarity)
        results = sorted(results, key=lambda r: r['score'], reverse=True)

        return {
            'results': results,
            'timestamp_ms': int(time.time() * 1000),
            'from_cache': False,
        }

    def precheck_filters(self, filter_kwargs, correlation_id):
        '''Check if a set of filter keyword arguments is acceptable.

        Some recommenders might not be able to produce any results
        if some filters are active; for instance, the tier 0
        recommender cannot enforce pre-filters on regions or dates,
        and so produces nothing if these filters are active.  This
        is a hook point that gives subclasses the ability to reject
        a recommendation request.

        :param dict filter_kwargs: additional pre-filtering keyword
          arguments to low-level scanning functions
        :param str correlation_id: session correlation ID, for
          logging purposes
        :return: :const:`True` if recommendations can continue

        '''
        return True

    @abc.abstractmethod
    def get_recommendations(self, _id, fc, filter_kwargs):
        '''Actually produce recommendations.

        Derived classes must implement this function (but may not need
        to implement any others).  The return value is an iterable of
        ordinary result dictionaries, with mandatory keys
        ``content_id`` and ``fc`` and optional keys ``confidence`` and
        ``relevance``, except that ``fc`` is an actual feature
        collection and not a serialization of it.

        :param str _id: identifier for the query item
        :param fc: query feature collection
        :type fc: :cls:`treelab.fc.FeatureCollection`
        :param dict filter_kwargs: additional pre-filtering keyword
          arguments to low-level scanning functions
        :return: iterable of result dictionaries

        '''


class Tier0Worker(RecommenderWorker):
    '''Worker that produces "tier 0" recommendation results.

    These are based on string match between the feature collection's
    `#NAME` field and candidate documents.  Since these are only based
    on simple string matches, they are fast.

    '''

    def __init__(self, connection, config, filters):
        '''Create a new tier 0 worker.'''
        super(Tier0Worker, self).__init__(connection, config, filters)

    @property
    def queue_name(self):
        '''Name of the queue this serves.'''
        return 'recommendations.fc.tier0'

    def precheck_filters(self, filter_kwargs, correlation_id):
        '''Check that the set of pre-filters is valid.

        This recommender never produces any results if there is a
        pubtime, evtime, or region filter.

        '''
        if (('filter_pubtime' in filter_kwargs or
             'filter_evtime' in filter_kwargs or
             'filter_region' in filter_kwargs)):
            LOGGER.info('no tier 0 results with pubtime, evtime, or '
                        'region filter '
                        'correlation_id=%s',
                        correlation_id)
            return False

        return True

    def get_recommendations(self, _, fc, filter_kwargs):
        '''Scan for plain-text matches and turn them into recommendations.'''
        queries = fc.get('#NAME', {}).keys()
        # TODO: setup an SI store:
        searched = self.si_store.search(queries, **filter_kwargs)

        fcs = imap(fc_from_elastic_result, searched)
        return ({
            'content_id': fc['#stream_id'].keys()[0],
            'fc': fc,
        } for fc in fcs)


if __name__ == '__main__':
    main()
